{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from tensorflow.python.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.python.keras.models import load_model\n",
    "from src.dataset_builders.text_data import create_easy_token_extraction_data\n",
    "from src.nn_settings.simple_gru_config import Config\n",
    "from src.simple_rnns import simple_gru, mikes_amount_rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Data set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/i862304/workspace/NeuralNetworkPlayground/notebooks/src/dataset_builders/text_data.py:59: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  sample_data = np.vstack((get_one_sample() for _ in range(data_size)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Vocabulary\n",
      "Building Encoding\n",
      "Setting Attributes\n",
      "Done\n",
      "input chars 9\n",
      "data size 50000\n",
      "[['dbXedYee' 'ed']\n",
      " ['adXbeYaa' 'be']\n",
      " ['bdXedYbd' 'ed']\n",
      " ['baXdbYae' 'db']\n",
      " ['ddXeaYdb' 'ea']]\n"
     ]
    }
   ],
   "source": [
    "config = Config()\n",
    "data = config.data\n",
    "num_input_characters = config.num_input_characters\n",
    "num_output_characters = config.num_output_characters\n",
    "input_encoding = config.input_encoding\n",
    "output_encoding = config.output_encoding\n",
    "enc = config.encoder\n",
    "max_output_length = enc.max_output_length\n",
    "\n",
    "print(\"input chars\", num_input_characters)\n",
    "print(\"data size\", len(data))\n",
    "print(data[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "temp_gru() missing 1 required positional argument: 'max_out_seq_len'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-5f94b37d0fd7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m                 \u001b[0mnum_output_characters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m                 \u001b[0mlatent_dim\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m                 max_output_length,)\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: temp_gru() missing 1 required positional argument: 'max_out_seq_len'"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.keras.models import Model, Sequential\n",
    "from tensorflow.python.keras.layers import Input, GRU, Dense, LSTM\n",
    "from tensorflow.python.keras.regularizers import l2\n",
    "from tensorflow.python.keras.layers import TimeDistributed, Bidirectional, RepeatVector\n",
    "from tensorflow.python.keras.optimizers import RMSprop\n",
    "\n",
    "def temp_gru(input_size, \n",
    "             output_size,\n",
    "             latent_dim,\n",
    "             max_input_length, \n",
    "             max_out_seq_len):\n",
    "    \"\"\"\n",
    "    A simple GRU stripped down about as much as possible\n",
    "    \"\"\"\n",
    "    inputs = Input(shape=(max_input_length, input_size,))\n",
    "    x = GRU(latent_dim,\n",
    "            return_sequences=False,\n",
    "            )(inputs)\n",
    "    x = RepeatVector(max_out_seq_len)(x)\n",
    "    x = Dense(output_size, activation=\"softmax\")(x)\n",
    "    outputs = TimeDistributed()(x)\n",
    "    model = Model(inputs, outputs)\n",
    "\n",
    "#     rms_prop = RMSprop(\n",
    "#                        rho=0.9,\n",
    "#                        decay=0.0,\n",
    "#                        epsilon=1e-08,\n",
    "#                        )\n",
    "#     model.compile(loss=\"categorical_crossentropy\",\n",
    "#                   optimizer=rms_prop)\n",
    "    return model\n",
    "\n",
    "\n",
    "latent_dim = 16\n",
    "model = temp_gru(num_input_characters,\n",
    "                num_output_characters,\n",
    "                latent_dim,\n",
    "                max_input_length, \n",
    "                max_output_length,)\n",
    "\n",
    "\n",
    "\n",
    "rms_prop = RMSprop(\n",
    "                   rho=0.9,\n",
    "                   decay=0.0,\n",
    "                   epsilon=1e-08,\n",
    "                   )\n",
    "model.compile(loss=\"categorical_crossentropy\",\n",
    "              optimizer=rms_prop)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru (GRU)                    (None, 128)               52992     \n",
      "_________________________________________________________________\n",
      "repeat_vector (RepeatVector) (None, 2, 128)            0         \n",
      "_________________________________________________________________\n",
      "time_distributed (TimeDistri (None, 2, 9)              1161      \n",
      "=================================================================\n",
      "Total params: 54,153\n",
      "Trainable params: 54,153\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "epochs = 50\n",
    "latent_dim = 128\n",
    "save_path = \"../data/keras_gru_token_extraction_2.h5\"\n",
    "\n",
    "if not os.path.isfile(save_path):\n",
    "    print(\"Creating Model\")\n",
    "    model = simple_gru(num_input_characters,\n",
    "               num_output_characters,\n",
    "               latent_dim,\n",
    "               max_output_length,\n",
    "              )\n",
    "else:\n",
    "    print(\"Loading Model\")\n",
    "    model = load_model(save_path)\n",
    "\n",
    "# define the checkpoint\n",
    "checkpoint = ModelCheckpoint(save_path, \n",
    "                             monitor='loss', \n",
    "                             verbose=1, \n",
    "                             save_best_only=True, \n",
    "                             mode='min')\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/50\n",
      "45000/45000 [==============================] - 25s 553us/step - loss: 0.5538 - val_loss: 0.5551\n",
      "Epoch 2/50\n",
      "45000/45000 [==============================] - 25s 556us/step - loss: 0.5538 - val_loss: 0.5554\n",
      "Epoch 3/50\n",
      "45000/45000 [==============================] - 25s 558us/step - loss: 0.5538 - val_loss: 0.5551\n",
      "Epoch 4/50\n",
      "45000/45000 [==============================] - 25s 558us/step - loss: 0.5538 - val_loss: 0.5550\n",
      "Epoch 5/50\n",
      "45000/45000 [==============================] - 25s 559us/step - loss: 0.5538 - val_loss: 0.5549\n",
      "Epoch 6/50\n",
      "45000/45000 [==============================] - 25s 558us/step - loss: 0.5538 - val_loss: 0.5553\n",
      "Epoch 7/50\n",
      "45000/45000 [==============================] - 25s 564us/step - loss: 0.5538 - val_loss: 0.5552\n",
      "Epoch 8/50\n",
      "45000/45000 [==============================] - 25s 557us/step - loss: 0.5538 - val_loss: 0.5551\n",
      "Epoch 9/50\n",
      "45000/45000 [==============================] - 25s 566us/step - loss: 0.5538 - val_loss: 0.5551\n",
      "Epoch 10/50\n",
      "45000/45000 [==============================] - 25s 550us/step - loss: 0.5538 - val_loss: 0.5552\n",
      "Epoch 11/50\n",
      "45000/45000 [==============================] - 25s 554us/step - loss: 0.5538 - val_loss: 0.5554\n",
      "Epoch 12/50\n",
      "45000/45000 [==============================] - 25s 559us/step - loss: 0.5538 - val_loss: 0.5550\n",
      "Epoch 13/50\n",
      "45000/45000 [==============================] - 25s 562us/step - loss: 0.5538 - val_loss: 0.5551\n",
      "Epoch 14/50\n",
      "45000/45000 [==============================] - 26s 578us/step - loss: 0.5538 - val_loss: 0.5554\n",
      "Epoch 15/50\n",
      "45000/45000 [==============================] - 25s 547us/step - loss: 0.5538 - val_loss: 0.5553\n",
      "Epoch 16/50\n",
      "45000/45000 [==============================] - 25s 566us/step - loss: 0.5538 - val_loss: 0.5551\n",
      "Epoch 17/50\n",
      "45000/45000 [==============================] - 25s 548us/step - loss: 0.5538 - val_loss: 0.5553\n",
      "Epoch 18/50\n",
      "45000/45000 [==============================] - 25s 555us/step - loss: 0.5538 - val_loss: 0.5554\n",
      "Epoch 19/50\n",
      "45000/45000 [==============================] - 25s 555us/step - loss: 0.5538 - val_loss: 0.5552\n",
      "Epoch 20/50\n",
      "45000/45000 [==============================] - 25s 558us/step - loss: 0.5538 - val_loss: 0.5551\n",
      "Epoch 21/50\n",
      "45000/45000 [==============================] - 25s 555us/step - loss: 0.5538 - val_loss: 0.5553\n",
      "Epoch 22/50\n",
      "45000/45000 [==============================] - 25s 558us/step - loss: 0.5538 - val_loss: 0.5557\n",
      "Epoch 23/50\n",
      "45000/45000 [==============================] - 25s 551us/step - loss: 0.5538 - val_loss: 0.5550\n",
      "Epoch 24/50\n",
      "45000/45000 [==============================] - 25s 547us/step - loss: 0.5538 - val_loss: 0.5551\n",
      "Epoch 25/50\n",
      "45000/45000 [==============================] - 25s 552us/step - loss: 0.5538 - val_loss: 0.5551\n",
      "Epoch 26/50\n",
      "45000/45000 [==============================] - 26s 573us/step - loss: 0.5538 - val_loss: 0.5552\n",
      "Epoch 27/50\n",
      "45000/45000 [==============================] - 26s 576us/step - loss: 0.5538 - val_loss: 0.5551\n",
      "Epoch 28/50\n",
      "45000/45000 [==============================] - 25s 559us/step - loss: 0.5538 - val_loss: 0.5552\n",
      "Epoch 29/50\n",
      "45000/45000 [==============================] - 25s 565us/step - loss: 0.5538 - val_loss: 0.5553\n",
      "Epoch 30/50\n",
      "45000/45000 [==============================] - 25s 565us/step - loss: 0.5538 - val_loss: 0.5551\n",
      "Epoch 31/50\n",
      "45000/45000 [==============================] - 25s 554us/step - loss: 0.5538 - val_loss: 0.5551\n",
      "Epoch 32/50\n",
      "45000/45000 [==============================] - 25s 552us/step - loss: 0.5538 - val_loss: 0.5553\n",
      "Epoch 33/50\n",
      "45000/45000 [==============================] - 25s 546us/step - loss: 0.5538 - val_loss: 0.5553\n",
      "Epoch 34/50\n",
      "45000/45000 [==============================] - 25s 553us/step - loss: 0.5538 - val_loss: 0.5551\n",
      "Epoch 35/50\n",
      "45000/45000 [==============================] - 25s 547us/step - loss: 0.5538 - val_loss: 0.5551\n",
      "Epoch 36/50\n",
      "45000/45000 [==============================] - 25s 546us/step - loss: 0.5538 - val_loss: 0.5551\n",
      "Epoch 37/50\n",
      "45000/45000 [==============================] - 26s 572us/step - loss: 0.5538 - val_loss: 0.5552\n",
      "Epoch 38/50\n",
      "45000/45000 [==============================] - 25s 560us/step - loss: 0.5538 - val_loss: 0.5551\n",
      "Epoch 39/50\n",
      "45000/45000 [==============================] - 25s 549us/step - loss: 0.5540 - val_loss: 0.5554\n",
      "Epoch 40/50\n",
      "45000/45000 [==============================] - 25s 552us/step - loss: 0.5538 - val_loss: 0.5552\n",
      "Epoch 41/50\n",
      "45000/45000 [==============================] - 25s 552us/step - loss: 0.5538 - val_loss: 0.5551\n",
      "Epoch 42/50\n",
      "45000/45000 [==============================] - 26s 574us/step - loss: 0.5538 - val_loss: 0.5553\n",
      "Epoch 43/50\n",
      "45000/45000 [==============================] - 25s 557us/step - loss: 0.5538 - val_loss: 0.5550\n",
      "Epoch 44/50\n",
      "45000/45000 [==============================] - 25s 545us/step - loss: 0.5538 - val_loss: 0.5551\n",
      "Epoch 45/50\n",
      "45000/45000 [==============================] - 26s 570us/step - loss: 0.5538 - val_loss: 0.5551\n",
      "Epoch 46/50\n",
      "45000/45000 [==============================] - 26s 584us/step - loss: 0.5538 - val_loss: 0.5550\n",
      "Epoch 47/50\n",
      "45000/45000 [==============================] - 26s 583us/step - loss: 0.5538 - val_loss: 0.5551\n",
      "Epoch 48/50\n",
      "45000/45000 [==============================] - 26s 573us/step - loss: 0.5538 - val_loss: 0.5550\n",
      "Epoch 49/50\n",
      "45000/45000 [==============================] - 26s 579us/step - loss: 0.5538 - val_loss: 0.5552\n",
      "Epoch 50/50\n",
      "45000/45000 [==============================] - 26s 582us/step - loss: 0.5538 - val_loss: 0.5550\n"
     ]
    }
   ],
   "source": [
    "# fit the model\n",
    "model.fit(input_encoding, output_encoding,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_split=0.1)\n",
    "model.save(save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try Mike's Amount RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use_embedding = False\n",
    "# max_out_seq_len = 10\n",
    "# hidden_size = 32\n",
    "# # build_model(len(rcpt_vocabulary), len(amt_vocabulary), MAX_DIGITS, 128, embedding=embedding)\n",
    "# model = mikes_amount_rnn(num_input_characters, num_output_characters, \n",
    "#                          max_out_seq_len=max_out_seq_len, hidden_size=hidden_size, \n",
    "#                          embedding=use_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(input_encoding.shape)\n",
    "print(output_encoding.shape)\n",
    "\n",
    "N = 43\n",
    "print(enc.decode(np.array([input_encoding[43]])))\n",
    "print(enc.decode(np.array([output_encoding[43]])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([b'cc'], dtype='|S8')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test_str = \"mqjrzvoXcorgiYaq snxoqkswccsaaahivlq\"\n",
    "test_str = \"aaXcaYbc\"\n",
    "test_enc = enc.encode([test_str], )\n",
    "\n",
    "test_prediction = model.predict(test_enc)\n",
    "# print(test_prediction)\n",
    "test_result = enc.decode(test_prediction)\n",
    "test_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# roll output data for teacher forcing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decoder_input_data is a 3D array of shape (num_pairs, max_french_sentence_length, num_french_characters) \n",
    "# containg a one-hot vectorization of the French sentences.\n",
    "# decoder_target_data is the same as decoder_input_data but offset by one timestep. \n",
    "# decoder_target_data[:, t, :] will be the same as decoder_input_data[:, t + 1, :].\n",
    "\n",
    "\n",
    "# temp_enc = Encoder(vocabulary_map)\n",
    "# temp = np.arange(0, 30).reshape((5, 3, 2))\n",
    "# temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n=0\n",
    "# print(\"target\")\n",
    "# print(temp[:, n, :])\n",
    "# print(\"input\")\n",
    "# print(temp[:, n+1, :])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
