{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "# import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Jupyter Imports\n"
     ]
    }
   ],
   "source": [
    "from src.main import prepare_data as prep_demo_data\n",
    "from src.config import Config\n",
    "from src.transformer import Transformer\n",
    "\n",
    "from src.utils import CustomSchedule\n",
    "from src.utils import LossObject\n",
    "from src.utils import create_masks\n",
    "from src.reids_data_loader.data_generator import DataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_i2a_data():\n",
    "    \n",
    "    pass\n",
    "    \n",
    "def prepare_data(config, dataset_type=\"PT2EN\"):\n",
    "    \"\"\"\n",
    "    PARAMS:\n",
    "    dataset_type (str): what dataset to prepare on of [\"I2A\", \"PT2EN\"]\n",
    "    \"\"\"\n",
    "    if dataset_type == \"I2A\":\n",
    "        train_dataset, val_dataset, tokenizer_in, tokenizer_out = prepare_i2a_data(config)\n",
    "        return train_dataset, val_dataset, tokenizer_in, tokenizer_out\n",
    "\n",
    "    if dataset_type == \"PT2EN\":\n",
    "        (train_dataset, val_dataset, \n",
    "         tokenizer_in, tokenizer_out) = prep_demo_data(config, \n",
    "                                                       \"../data/en_tokenizer\", \n",
    "                                                       \"../data/pt_tokenizer\")\n",
    "        return train_dataset, val_dataset, tokenizer_in, tokenizer_out\n",
    "    \n",
    "    \n",
    "def train_transformer(model, config, data=None):\n",
    "    \"\"\"\n",
    "    Transformer training code\n",
    "    \n",
    "    data = (train_dataset, input_vocab_size, target_vocab_size)\n",
    "    \"\"\"\n",
    "    log_dir = config.log_dir\n",
    "\n",
    "    BUFFER_SIZE = config.BUFFER_SIZE\n",
    "    BATCH_SIZE = config.BATCH_SIZE\n",
    "    EPOCHS = 1 # config.EPOCHS\n",
    "    MAX_LENGTH = config.MAX_LENGTH\n",
    "\n",
    "    # Hyperparameters\n",
    "    num_layers = config.num_layers\n",
    "    d_model = config.d_model\n",
    "    dff = config.dff\n",
    "    num_heads = config.num_heads\n",
    "    dropout_rate = config.dropout_rate\n",
    "\n",
    "    if data is None:\n",
    "        print(\"preparing data ...\")\n",
    "        (train_dataset, val_dataset,\n",
    "         tokenizer_en, tokenizer_pt) = prepare_data(config)\n",
    "\n",
    "        input_vocab_size = tokenizer_pt.vocab_size + 2\n",
    "        target_vocab_size = tokenizer_en.vocab_size + 2\n",
    "        \n",
    "    else:\n",
    "        (train_dataset, input_vocab_size, target_vocab_size) = data\n",
    "        \n",
    "#     transformer = Transformer(num_layers, d_model, num_heads, dff,\n",
    "#                           input_vocab_size, target_vocab_size,\n",
    "#                           pe_input=input_vocab_size,\n",
    "#                           pe_target=target_vocab_size,\n",
    "#                           rate=dropout_rate)\n",
    "    transformer = model\n",
    "\n",
    "    train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "    train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(\n",
    "        name='train_accuracy')\n",
    "\n",
    "    learning_rate = CustomSchedule(d_model)\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate,\n",
    "                                         beta_1=0.9,\n",
    "                                         beta_2=0.98,\n",
    "                                         epsilon=1e-9)\n",
    "    loss_function = LossObject().loss_function\n",
    "\n",
    "    checkpoint_path = \"./checkpoints/train\"\n",
    "\n",
    "    ckpt = tf.train.Checkpoint(transformer=transformer,\n",
    "                               optimizer=optimizer)\n",
    "\n",
    "    ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
    "\n",
    "    # if a checkpoint exists, restore the latest checkpoint.\n",
    "    if ckpt_manager.latest_checkpoint:\n",
    "        # ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "        ckpt.restore(ckpt_manager.latest_checkpoint).expect_partial()\n",
    "        print ('Latest checkpoint restored!!')\n",
    "\n",
    "    ## set up logging of Tensorboard events\n",
    "    current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    train_log_dir = os.path.join(log_dir, \"events\", current_time, \"train\")\n",
    "    # test_log_dir = log_dir + current_time + '/test'\n",
    "    train_summary_writer = tf.summary.create_file_writer(train_log_dir)\n",
    "    # test_summary_writer = tf.summary.create_file_writer(test_log_dir)\n",
    "\n",
    "    train_step_signature = [\n",
    "        tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n",
    "        tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n",
    "    ]\n",
    "\n",
    "    @tf.function(input_signature=train_step_signature)\n",
    "    def train_step(inp, tar):\n",
    "        tar_inp = tar[:, :-1]\n",
    "        tar_real = tar[:, 1:]\n",
    "\n",
    "        (enc_padding_mask,\n",
    "         combined_mask,\n",
    "         dec_padding_mask) = create_masks(inp, tar_inp)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions, _ = transformer(inp, tar_inp,\n",
    "                                         True,\n",
    "                                         enc_padding_mask,\n",
    "                                         combined_mask,\n",
    "                                         dec_padding_mask)\n",
    "            loss = loss_function(tar_real, predictions)\n",
    "\n",
    "        gradients = tape.gradient(loss, transformer.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients,\n",
    "                                      transformer.trainable_variables))\n",
    "\n",
    "        train_loss(loss)\n",
    "        train_accuracy(tar_real, predictions)\n",
    "\n",
    "\n",
    "    # def test_step(model, x_test, y_test):\n",
    "    #     predictions = model(x_test)\n",
    "    #     loss = loss_object(y_test, predictions)\n",
    "    #\n",
    "    #     test_loss(loss)\n",
    "    #     test_accuracy(y_test, predictions)\n",
    "\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        start = time.time()\n",
    "\n",
    "        train_loss.reset_states()\n",
    "        train_accuracy.reset_states()\n",
    "\n",
    "        # inp -> portuguese, tar -> english\n",
    "        for (batch, (inp, tar)) in enumerate(train_dataset):\n",
    "            train_step(inp, tar)\n",
    "\n",
    "            ## Tensorboard event logging\n",
    "            ### QUESTION: should this be less often?\n",
    "            with train_summary_writer.as_default():\n",
    "                tf.summary.scalar('loss',\n",
    "                                  train_loss.result(),\n",
    "                                  step=epoch)\n",
    "                tf.summary.scalar('accuracy',\n",
    "                                  train_accuracy.result(),\n",
    "                                  step=epoch)\n",
    "\n",
    "            if batch % 50 == 0:\n",
    "                print ('Epoch {} Batch {} Loss {:.4f} Accuracy {:.4f}'.format(\n",
    "                       epoch + 1, batch, train_loss.result(), train_accuracy.result()))\n",
    "\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            ckpt_save_path = ckpt_manager.save()\n",
    "            print ('Saving checkpoint for epoch {} at {}'.format(epoch+1,\n",
    "                                                             ckpt_save_path))\n",
    "\n",
    "        print ('Epoch {} Loss {:.4f} Accuracy {:.4f}'.format(epoch + 1,\n",
    "                                                    train_loss.result(),\n",
    "                                                    train_accuracy.result()))\n",
    "\n",
    "        print ('Time taken for 1 epoch: {} secs\\n'.format(time.time() - start))\n",
    "\n",
    "    print(\"We got to here!!!\")\n",
    "    \n",
    "    \n",
    "def evaluate(inp_sentence):\n",
    "    print(\"In evaluate()\")\n",
    "    start_token = [tokenizer_pt.vocab_size]\n",
    "    end_token = [tokenizer_pt.vocab_size + 1]\n",
    "\n",
    "    # inp sentence is portuguese, hence adding the start and end token\n",
    "    inp_sentence = start_token + tokenizer_pt.encode(inp_sentence) + end_token\n",
    "    print(\"input sentence encoded\")\n",
    "    encoder_input = tf.expand_dims(inp_sentence, 0)\n",
    "    \n",
    "    print(\"len(inp_sentence)\", len(inp_sentence))\n",
    "    print(\"shape of encoded input\", encoder_input.shape)\n",
    "\n",
    "    # as the target is english, the first word to the transformer should be the\n",
    "    # english start token.\n",
    "    decoder_input = [tokenizer_en.vocab_size]\n",
    "    output = tf.expand_dims(decoder_input, 0)\n",
    "\n",
    "    for i in range(MAX_LENGTH):\n",
    "        if i % 50 == 0:\n",
    "            print(\"at step\", i)\n",
    "            \n",
    "        enc_padding_mask, combined_mask, dec_padding_mask = create_masks(\n",
    "                                                                encoder_input, output)\n",
    "\n",
    "        # predictions.shape == (batch_size, seq_len, vocab_size)\n",
    "        predictions, attention_weights = transformer(encoder_input, \n",
    "                                                     output,\n",
    "                                                     False,\n",
    "                                                     enc_padding_mask,\n",
    "                                                     combined_mask,\n",
    "                                                     dec_padding_mask)\n",
    "\n",
    "        # select the last word from the seq_len dimension\n",
    "        predictions = predictions[: ,-1:, :]  # (batch_size, 1, vocab_size)\n",
    "\n",
    "        predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
    "\n",
    "        # return the result if the predicted_id is equal to the end token\n",
    "        if predicted_id == tokenizer_en.vocab_size+1:\n",
    "            return tf.squeeze(output, axis=0), attention_weights\n",
    "    \n",
    "        # concatentate the predicted_id to the output which is given to the decoder\n",
    "        # as its input.\n",
    "        output = tf.concat([output, predicted_id], axis=-1)\n",
    "\n",
    "    return tf.squeeze(output, axis=0), attention_weights\n",
    "\n",
    "\n",
    "def translate(sentence, plot=''):\n",
    "    result, attention_weights = evaluate(sentence)\n",
    "\n",
    "    predicted_sentence = tokenizer_en.decode([i for i in result \n",
    "                                            if i < tokenizer_en.vocab_size])  \n",
    "\n",
    "    print('Input: {}'.format(sentence))\n",
    "    print('Predicted translation: {}'.format(predicted_sentence))\n",
    "  \n",
    "    if plot:\n",
    "        plot_attention_weights(attention_weights, sentence, result, plot)\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare model, data and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading dataset\n",
      "building tokenizers\n",
      "filtering and encoding datasets\n",
      "Complete\n"
     ]
    }
   ],
   "source": [
    "train_dataset, val_dataset, tokenizer_en, tokenizer_pt = prepare_data(config, dataset_type=\"PT2EN\")\n",
    "print(\"Complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Jupyter Imports\n"
     ]
    }
   ],
   "source": [
    "en_voc_path = \"../data/en_tokenizer\"\n",
    "tokenizer_en.save_to_file(en_voc_path)\n",
    "\n",
    "pt_voc_path = \"../data/pt_tokenizer\"\n",
    "tokenizer_pt.save_to_file(pt_voc_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating Transformer\n"
     ]
    }
   ],
   "source": [
    "MAX_LENGTH = config.MAX_LENGTH\n",
    "\n",
    "num_layers = config.num_layers\n",
    "d_model = config.d_model\n",
    "num_heads = config.num_heads\n",
    "dff = config.dff\n",
    "dropout_rate = config.dropout_rate\n",
    "\n",
    "\n",
    "# def build_transformer(train_dataset, val_dataset, tokenizer_pt, tokenizer_en,):\n",
    "input_vocab_size = tokenizer_pt.vocab_size + 2\n",
    "target_vocab_size = tokenizer_en.vocab_size + 2\n",
    "# input_vocab_size = config.input_vocab_size\n",
    "# target_vocab_size = config.target_vocab_size\n",
    "# input_vocab_size = 1\n",
    "# target_vocab_size = 12\n",
    "\n",
    "\n",
    "print(\"creating Transformer\")\n",
    "transformer = Transformer(num_layers, d_model, num_heads, dff,\n",
    "                      input_vocab_size, target_vocab_size,\n",
    "                      pe_input=input_vocab_size,\n",
    "                      pe_target=target_vocab_size,\n",
    "                      rate=dropout_rate)\n",
    "\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(\n",
    "    name='train_accuracy')\n",
    "\n",
    "learning_rate = CustomSchedule(d_model)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate,\n",
    "                                     beta_1=0.9,\n",
    "                                     beta_2=0.98,\n",
    "                                     epsilon=1e-9)\n",
    "loss_function = LossObject().loss_function\n",
    "    \n",
    "# transformer = build_transformer(train_dataset, val_dataset, tokenizer_pt, tokenizer_en,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examine data and encoder output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In evaluate()\n",
      "input sentence encoded\n",
      "len(inp_sentence) 4\n",
      "shape of encoded input (1, 4)\n",
      "<class 'tuple'>\n"
     ]
    }
   ],
   "source": [
    "output = evaluate(\"do gato\")\n",
    "print(type(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8087  501]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'turns '"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(output[0].numpy())\n",
    "\n",
    "predicted_sentence = tokenizer_en.decode([i for i in output[0] \n",
    "                                            if i < tokenizer_en.vocab_size])\n",
    "predicted_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the  , eat appearbootthe who '"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_en.decode([3, 1, 1037, 2053, 7777, 3, 82, ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 (64, 60) (64, 62)\n",
      "2 (64, 79) (64, 71)\n",
      "2 (64, 80) (64, 84)\n",
      "2 (64, 85) (64, 82)\n",
      "2 (64, 85) (64, 82)\n",
      "2 (64, 62) (64, 66)\n"
     ]
    }
   ],
   "source": [
    "for ii, item in enumerate(train_dataset):\n",
    "    if ii > 5: break\n",
    "    print(len(item), item[0].numpy().shape, item[1].numpy().shape, sep=\" \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([8087,   62,   12,  101,   10,   16,   13,   36, 1395,   52,  537,\n",
       "          5,    3, 5396,   45,   11,    3,  135, 1973,  104,    4,  252,\n",
       "       2264,  129,  137,    1,   16, 2437,    1,   16,  908,    1,   29,\n",
       "        149,   16,  333,  798, 8088,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_item[1].numpy()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"`` i know that this is not easy . turn to the gentleman in the little suite and say : `` look here , this contract , this part , what does this mean ? '' ''\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_en.decode([xx for xx in sample_item[1].numpy()[0] if xx < 8087])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tuple"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = next(iter(train_dataset))\n",
    "type(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(62,)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp[0].numpy()[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# run single example through transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _evaluate(inp_sentence, model):\n",
    "#     start_token = [tokenizer_pt.vocab_size]\n",
    "#     end_token = [tokenizer_pt.vocab_size + 1]\n",
    "\n",
    "#     # inp sentence is portuguese, hence adding the start and end token\n",
    "#     inp_sentence = start_token + tokenizer_pt.encode(inp_sentence) + end_token\n",
    "\n",
    "    transformer = model\n",
    "    encoder_input = tf.expand_dims(inp_sentence, 0)\n",
    "    print(encoder_input.shape)\n",
    "#     encoder_input = inp_sentence\n",
    "\n",
    "    # as the target is english, the first word to the transformer should be the\n",
    "    # english start token.\n",
    "    decoder_input = [tokenizer_en.vocab_size]\n",
    "    output = tf.expand_dims(decoder_input, 0)\n",
    "\n",
    "#     enc_padding_mask, combined_mask, dec_padding_mask = create_masks(\n",
    "#                                                             encoder_input, output)\n",
    "    \n",
    "    for i in range(MAX_LENGTH):\n",
    "#     for i in range(3):\n",
    "\n",
    "        if i % 20 == 0:\n",
    "            print(\"at step {} of {}\".format(i, MAX_LENGTH))\n",
    "        enc_padding_mask, combined_mask, dec_padding_mask = create_masks(\n",
    "                                                                encoder_input, output)\n",
    "\n",
    "        # predictions.shape == (batch_size, seq_len, vocab_size)\n",
    "        predictions, attention_weights = transformer(encoder_input, \n",
    "                                                     output,\n",
    "                                                     False,\n",
    "                                                     enc_padding_mask,\n",
    "                                                     combined_mask,\n",
    "                                                     dec_padding_mask)\n",
    "\n",
    "        # select the last word from the seq_len dimension\n",
    "        predictions = predictions[: ,-1:, :]  # (batch_size, 1, vocab_size)\n",
    "\n",
    "        predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
    "\n",
    "        # return the result if the predicted_id is equal to the end token\n",
    "        if predicted_id == tokenizer_en.vocab_size+1:\n",
    "            return tf.squeeze(output, axis=0), attention_weights\n",
    "    \n",
    "        # concatentate the predicted_id to the output which is given to the decoder\n",
    "        # as its input.\n",
    "        output = tf.concat([output, predicted_id], axis=-1)\n",
    "\n",
    "    return tf.squeeze(output, axis=0), attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: id=637054, shape=(64, 100), dtype=int64, numpy=\n",
       " array([[8214,    7, 2341, ...,    0,    0,    0],\n",
       "        [8214,   18,  322, ...,    0,    0,    0],\n",
       "        [8214,   22, 7189, ...,    0,    0,    0],\n",
       "        ...,\n",
       "        [8214, 8059,    1, ...,    0,    0,    0],\n",
       "        [8214,  276,    1, ...,    0,    0,    0],\n",
       "        [8214,    3,  496, ...,    0,    0,    0]])>,\n",
       " <tf.Tensor: id=637055, shape=(64, 87), dtype=int64, numpy=\n",
       " array([[8087,    3,  706, ...,    0,    0,    0],\n",
       "        [8087,  368,   97, ...,    0,    0,    0],\n",
       "        [8087,   11,    3, ...,    0,    0,    0],\n",
       "        ...,\n",
       "        [8087,  359,    1, ...,    0,    0,    0],\n",
       "        [8087,   23,  145, ...,    0,    0,    0],\n",
       "        [8087,    3,  486, ...,    0,    0,    0]])>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_item = next(iter(train_dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 100)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_item[0].numpy().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 65) (64, 61)\n",
      "(65,) (61,)\n",
      "(1, 65)\n",
      "at step 0 of 400\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "sample_item = next(iter(train_dataset))\n",
    "\n",
    "# # inp, target = next(data)\n",
    "inp, tar_inp = sample_item\n",
    "\n",
    "\n",
    "# (enc_padding_mask,\n",
    "#  combined_mask,\n",
    "#  dec_padding_mask) = create_masks(inp, tar_inp)\n",
    "\n",
    "print(inp.shape, tar_inp.shape)\n",
    "\n",
    "\n",
    "inp, tar_inp = inp[0], tar_inp[0]\n",
    "print(inp.shape, tar_inp.shape)\n",
    "\n",
    "# (enc_padding_mask,\n",
    "#  combined_mask,\n",
    "#  dec_padding_mask) = create_masks(inp, tar_inp)\n",
    "\n",
    "# print(enc_padding_mask.shape,\n",
    "#  combined_mask.shape,\n",
    "#  dec_padding_mask.shape)\n",
    "\n",
    "\n",
    "temp = _evaluate(inp, transformer)\n",
    "\n",
    "print(\"Done\")\n",
    "\n",
    "# # predictions, _ = transformer(inp, tar_inp,\n",
    "# #                              False,\n",
    "# #                              enc_padding_mask,\n",
    "# #                              combined_mask,\n",
    "# #                              dec_padding_mask)\n",
    "\n",
    "# # predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([8087], dtype=int32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp[0].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['decoder_layer1_block1', 'decoder_layer1_block2', 'decoder_layer2_block1', 'decoder_layer2_block2', 'decoder_layer3_block1', 'decoder_layer3_block2', 'decoder_layer4_block1', 'decoder_layer4_block2'])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp[1].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latest checkpoint restored!!\n",
      "Epoch 1 Batch 0 Loss 0.1181 Accuracy 0.3477\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-2afcce179c5f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_vocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_vocab_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_transformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-29-799b1bff8474>\u001b[0m in \u001b[0;36mtrain_transformer\u001b[0;34m(model, config, data)\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0;31m# inp -> portuguese, tar -> english\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m             \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m             \u001b[0;31m## Tensorboard event logging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_counter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalled_without_tracing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    485\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 487\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    488\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1821\u001b[0m     \u001b[0;34m\"\"\"Calls a graph function specialized to the inputs.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1822\u001b[0m     \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1823\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1824\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1825\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1139\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1140\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1141\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1143\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1222\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1223\u001b[0m       flat_outputs = forward_function.call(\n\u001b[0;32m-> 1224\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1225\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1226\u001b[0m       \u001b[0mgradient_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delayed_rewrite_functions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    509\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    512\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "data = (train_dataset, input_vocab_size, target_vocab_size)\n",
    "train_transformer(transformer, config, data=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In evaluate()\n",
      "input sentence encoded\n",
      "len(inp_sentence) 11\n",
      "shape of encoded input (1, 11)\n",
      "at step 0\n",
      "Input: este é um problema que temos que resolver.\n",
      "Predicted translation: this is one problem that we have to solve precisely by the time in data .\n",
      "Real translation: this is a problem we have to solve .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "translate(\"este é um problema que temos que resolver.\")\n",
    "print (\"Real translation: this is a problem we have to solve .\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In evaluate()\n",
      "input sentence encoded\n",
      "len(inp_sentence) 11\n",
      "shape of encoded input (1, 11)\n",
      "at step 0\n",
      "Input: os meus vizinhos ouviram sobre esta ideia.\n",
      "Predicted translation: my neighbors have heard about such a idea .\n",
      "Real translation: and my neighboring homes heard about this idea .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "translate(\"os meus vizinhos ouviram sobre esta ideia.\")\n",
    "print (\"Real translation: and my neighboring homes heard about this idea .\")\n",
    "print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In evaluate()\n",
      "input sentence encoded\n",
      "len(inp_sentence) 20\n",
      "shape of encoded input (1, 20)\n",
      "at step 0\n",
      "Input: vou então muito rapidamente partilhar convosco algumas histórias de algumas coisas mágicas que aconteceram.\n",
      "Predicted translation: so i 'm very quickly going to share with you some stories of some magic things that happened so much as am .\n",
      "Real translation: so i 'll just share with you some stories very quickly of some magical things that have happened .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "translate(\"vou então muito rapidamente partilhar convosco algumas histórias de algumas coisas mágicas que aconteceram.\")\n",
    "print (\"Real translation: so i 'll just share with you some stories very quickly of some magical things that have happened .\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In evaluate()\n",
      "input sentence encoded\n",
      "len(inp_sentence) 11\n",
      "shape of encoded input (1, 11)\n",
      "at step 0\n",
      "Input: este é o primeiro livro que eu fiz.\n",
      "Predicted translation: this is the first book i ever did .\n",
      "Real translation: this is the first book i've ever done.\n"
     ]
    }
   ],
   "source": [
    "translate(\"este é o primeiro livro que eu fiz.\") #, plot='decoder_layer4_block2')\n",
    "print (\"Real translation: this is the first book i've ever done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
