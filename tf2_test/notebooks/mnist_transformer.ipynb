{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "# import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Jupyter Imports\n"
     ]
    }
   ],
   "source": [
    "from src.main import prepare_data as prep_demo_data\n",
    "from src.config import Config\n",
    "# from src.transformer import Transformer\n",
    "from src.hacked_transformer import Transformer\n",
    "\n",
    "from src.utils import CustomSchedule\n",
    "from src.utils import LossObject\n",
    "from src.utils import create_masks\n",
    "from src.reids_data_loader.data_generator import DataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_i2a_data():\n",
    "    \n",
    "    pass\n",
    "    \n",
    "def prepare_data(config, dataset_type=\"I2A\"):\n",
    "    \"\"\"\n",
    "    PARAMS:\n",
    "    dataset_type (str): what dataset to prepare on of [\"I2A\", \"PT2EN\"]\n",
    "    \"\"\"\n",
    "    if dataset_type == \"I2A\":\n",
    "        train_dataset, val_dataset, tokenizer_in, tokenizer_out = prepare_i2a_data(config)\n",
    "        return train_dataset, val_dataset, tokenizer_in, tokenizer_out\n",
    "\n",
    "    if dataset_type == \"PT2EN\":\n",
    "        (train_dataset, val_dataset, \n",
    "         tokenizer_in, tokenizer_out) = prep_demo_data(config, \n",
    "                                                       \"./tok_en.subwords\",\n",
    "                                                       \"./tok_pt.subwords\",\n",
    "#                                                        \"../data/en_tokenizer\", \n",
    "#                                                        \"../data/pt_tokenizer\",\n",
    "                                                      )\n",
    "        return train_dataset, val_dataset, tokenizer_in, tokenizer_out\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def translate(sentence, plot=''):\n",
    "    result, attention_weights = evaluate(sentence)\n",
    "\n",
    "    predicted_sentence = tokenizer_en.decode([i for i in result \n",
    "                                            if i < tokenizer_en.vocab_size])  \n",
    "\n",
    "    print('Input: {}'.format(sentence))\n",
    "    print('Predicted translation: {}'.format(predicted_sentence))\n",
    "  \n",
    "    if plot:\n",
    "        plot_attention_weights(attention_weights, sentence, result, plot)\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# demo_train_dataset, demo_val_dataset, demo_tokenizer_en, demo_tokenizer_pt = prepare_data(config, dataset_type=\"PT2EN\")\n",
    "# print(\"Complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load MNIST keras-style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.python.keras.utils import to_categorical\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28, 1)\n",
      "(10000, 28, 28, 1)\n",
      "(60000,)\n",
      "(10000,)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "IMG_SHAPE = (28, 28, 1)\n",
    "NUM_CLASSES = 10\n",
    "img_rows = IMG_SHAPE[0]\n",
    "img_cols = IMG_SHAPE[1]\n",
    "\n",
    "## Load Data:\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "# data = np.load(path_data)\n",
    "# (x_train, y_train), (x_test, y_test) = (data[\"x_train\"], data[\"y_train\"]), (data[\"x_test\"], data[\"y_test\"])\n",
    "\n",
    "# Normalize and Reshape Data:\n",
    "# x_train = x_train.astype('float32') / 255.\n",
    "# x_test = x_test.astype('float32') / 255.\n",
    "\n",
    "# y_train = to_categorical(y_train, NUM_CLASSES)\n",
    "# y_test = to_categorical(y_test, NUM_CLASSES)\n",
    "\n",
    "x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "# input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mnist_numpy_generator(epochs=3, batch_size=30):\n",
    "#     temp = np.arange(90).reshape(10, 3, 3, 1)\n",
    "#     print(len(temp))\n",
    "#     print(temp.shape)\n",
    "# #     print(temp)\n",
    "\n",
    "#     temp = temp.reshape(5, 2, -1)\n",
    "    \n",
    "#     for ii in temp:\n",
    "#         print(type(ii))\n",
    "#         print(ii)\n",
    "#         print(\"---\")\n",
    "\n",
    "    ## Flatten and batch images\n",
    "    ## Batchsize must be a factor of len(x_train)\n",
    "#     xx = x_train.reshape(len(x_train)//batch_size, batch_size, 1, -1)\n",
    "#     yy = y_train.reshape(len(y_train)//batch_size, batch_size, 1, -1)\n",
    "    xx = x_train.reshape(len(x_train)//batch_size, batch_size, -1)\n",
    "    yy = y_train.reshape(len(y_train)//batch_size, batch_size, -1)\n",
    "    for _ in range(epochs):\n",
    "        for batch in zip(xx, yy):\n",
    "            yield batch\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[300.,   1.,  73., 240.,   1.,  48., 205.,   1., 215.,  50., 301.],\n",
       "        [300.,   1., 155., 183.,   1., 123.,  17.,   1.,  59.,   7., 301.],\n",
       "        [300.,   1., 139., 280.,   1.,  25.,   5.,   1., 259., 184., 301.],\n",
       "        [300.,   1., 166.,  14.,   1.,  40., 234.,   1., 234.,  49., 301.]],\n",
       "       dtype=float32), array([[3., 1., 1., 1., 2., 4.],\n",
       "        [3., 2., 1., 2., 1., 4.],\n",
       "        [3., 1., 2., 1., 2., 4.],\n",
       "        [3., 1., 1., 2., 1., 4.]], dtype=float32))"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# def fake_data_gen(dataset_len=40, batch_size=4):\n",
    "#     assert dataset_len % batch_size == 0, \"batch_size must divide evenly into dataset_len\"\n",
    "#     L = dataset_len # len dataset\n",
    "#     x_size = (3, 3)\n",
    "#     y_size = 4\n",
    "#     xx_data_range = 300\n",
    "#     yy_data_range = 3\n",
    "#     batch_size = batch_size # must divide evenly into L\n",
    "#     batch_num = L//batch_size\n",
    "    \n",
    "# #     xx = np.random.randint(0, xx_data_range, (L, x_size[0], x_size[1])) #, dtype=\"float32\")\n",
    "# #     xx[:, :, 0] = 0\n",
    "# #     yy = np.random.randint(0, yy_data_range, (L, y_size))\n",
    "\n",
    "#     xx = np.random.randint(1, xx_data_range, (L, x_size[0], x_size[1])) #, dtype=\"float32\")\n",
    "#     xx[:, :, 0] = 1\n",
    "#     yy = np.random.randint(1, yy_data_range, (L, y_size))    \n",
    "    \n",
    "#     def _add_start_stop_tokens(data, data_range, batch_num, batch_size):\n",
    "#         data = data.reshape(batch_num, batch_size, -1)\n",
    "#         temp_start = data_range * np.ones((batch_num, batch_size, 1))\n",
    "#         temp_stop = data_range * np.ones((batch_num, batch_size, 1)) + 1\n",
    "#         output = np.concatenate([temp_start, data, temp_stop], axis=2)\n",
    "#         output = output.astype(dtype=\"float32\")\n",
    "#         return output\n",
    "# #     temp = xx\n",
    "#     xx = _add_start_stop_tokens(xx, xx_data_range, batch_num, batch_size)\n",
    "#     yy = _add_start_stop_tokens(yy, yy_data_range, batch_num, batch_size)\n",
    "#     for item in zip(xx, yy):\n",
    "# #         print(temp.dtype)        \n",
    "# #         print(item[0].dtype)\n",
    "#         yield item\n",
    "\n",
    "# next(fake_data_gen())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[300.,   1., 290.,   5.,   1., 283., 147.,   1., 154.,  72., 301.],\n",
       "        [300.,   1., 197., 202.,   1., 190., 228.,   1., 252., 192., 301.],\n",
       "        [300.,   1., 158.,  43.,   1., 175., 226.,   1.,  17., 299., 301.],\n",
       "        [300.,   1., 169., 286.,   1., 123., 160.,   1., 164., 238., 301.]],\n",
       "       dtype=float32), array([[3., 2., 1., 2., 1., 4.],\n",
       "        [3., 1., 2., 1., 1., 4.],\n",
       "        [3., 2., 1., 2., 2., 4.],\n",
       "        [3., 2., 2., 1., 1., 4.]], dtype=float32))"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class FakeData:\n",
    "    def __init__(self, dataset_len=40, batch_size=4):\n",
    "        assert dataset_len % batch_size == 0, \"batch_size must divide evenly into dataset_len\"\n",
    "        L = dataset_len # len dataset\n",
    "        x_size = (3, 3)\n",
    "        y_size = 4\n",
    "        xx_data_range = 300\n",
    "        yy_data_range = 3\n",
    "        batch_size = batch_size # must divide evenly into L\n",
    "        batch_num = L//batch_size\n",
    "\n",
    "    #     xx = np.random.randint(0, xx_data_range, (L, x_size[0], x_size[1])) #, dtype=\"float32\")\n",
    "    #     xx[:, :, 0] = 0\n",
    "    #     yy = np.random.randint(0, yy_data_range, (L, y_size))\n",
    "\n",
    "        xx = np.random.randint(1, xx_data_range, (L, x_size[0], x_size[1])) #, dtype=\"float32\")\n",
    "        xx[:, :, 0] = 1\n",
    "        yy = np.random.randint(1, yy_data_range, (L, y_size))    \n",
    "\n",
    "        def _add_start_stop_tokens(data, data_range, batch_num, batch_size):\n",
    "            data = data.reshape(batch_num, batch_size, -1)\n",
    "            temp_start = data_range * np.ones((batch_num, batch_size, 1))\n",
    "            temp_stop = data_range * np.ones((batch_num, batch_size, 1)) + 1\n",
    "            output = np.concatenate([temp_start, data, temp_stop], axis=2)\n",
    "            output = output.astype(dtype=\"float32\")\n",
    "            return output\n",
    "    #     temp = xx\n",
    "        self.xx = _add_start_stop_tokens(xx, xx_data_range, batch_num, batch_size)\n",
    "        self.yy = _add_start_stop_tokens(yy, yy_data_range, batch_num, batch_size)\n",
    "        \n",
    "    def batch_generator(self):\n",
    "        for item in zip(self.xx, self.yy):\n",
    "            # print(temp.dtype)        \n",
    "            # print(item[0].dtype)\n",
    "            yield item\n",
    "    \n",
    "\n",
    "next(FakeData().batch_generator())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tuple'> (4, 11) (4, 6)\n",
      "<class 'tuple'> (4, 11) (4, 6)\n",
      "<class 'tuple'> (4, 11) (4, 6)\n",
      "<class 'tuple'> (4, 11) (4, 6)\n",
      "<class 'tuple'> (4, 11) (4, 6)\n",
      "<class 'tuple'> (4, 11) (4, 6)\n"
     ]
    }
   ],
   "source": [
    "# my_gen = mnist_numpy_generator()\n",
    "my_gen = FakeData().batch_generator()\n",
    "\n",
    "for ii, item in enumerate(my_gen):\n",
    "    if ii > 5: break\n",
    "    print(type(item), item[0].shape, item[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3., 2., 2., 1., 2., 4.], dtype=float32)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_item = next(fake_data_gen())\n",
    "sample_item[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mnist_train_transformer(model, config, data=None):\n",
    "    \"\"\"\n",
    "    Transformer training code\n",
    "    \"\"\"\n",
    "    log_dir = \"../data/logs/\"\n",
    "\n",
    "#     BUFFER_SIZE = config.BUFFER_SIZE\n",
    "#     BATCH_SIZE = config.BATCH_SIZE\n",
    "    EPOCHS = 3 # config.EPOCHS\n",
    "#     MAX_LENGTH = config.MAX_LENGTH\n",
    "\n",
    "    # Hyperparameters\n",
    "    num_layers = config.num_layers\n",
    "    d_model = config.d_model\n",
    "    dff = config.dff\n",
    "    num_heads = config.num_heads\n",
    "    dropout_rate = config.dropout_rate\n",
    "\n",
    "    print(\"preparing data ...\")\n",
    "    train_dataset = data\n",
    "\n",
    "    transformer = model\n",
    "\n",
    "    train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "    train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(\n",
    "        name='train_accuracy')\n",
    "\n",
    "    learning_rate = CustomSchedule(d_model)\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate,\n",
    "                                         beta_1=0.9,\n",
    "                                         beta_2=0.98,\n",
    "                                         epsilon=1e-9)\n",
    "    loss_function = LossObject().loss_function\n",
    "\n",
    "    checkpoint_path = \"./checkpoints/train\"\n",
    "\n",
    "    ckpt = tf.train.Checkpoint(transformer=transformer,\n",
    "                               optimizer=optimizer)\n",
    "\n",
    "    ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
    "\n",
    "    # if a checkpoint exists, restore the latest checkpoint.\n",
    "    if ckpt_manager.latest_checkpoint:\n",
    "        # ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "        ckpt.restore(ckpt_manager.latest_checkpoint).expect_partial()\n",
    "        print ('Latest checkpoint restored!!')\n",
    "\n",
    "    ## set up logging of Tensorboard events\n",
    "    current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    train_log_dir = os.path.join(log_dir, \"events\", current_time, \"train\")\n",
    "    # test_log_dir = log_dir + current_time + '/test'\n",
    "    train_summary_writer = tf.summary.create_file_writer(train_log_dir)\n",
    "    # test_summary_writer = tf.summary.create_file_writer(test_log_dir)\n",
    "\n",
    "    train_step_signature = [\n",
    "        tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n",
    "        tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n",
    "    ]\n",
    "\n",
    "    @tf.function(input_signature=train_step_signature)\n",
    "    def train_step(inp, tar):\n",
    "        tar_inp = tar[:, :-1]\n",
    "        tar_real = tar[:, 1:]\n",
    "\n",
    "        (enc_padding_mask,\n",
    "         combined_mask,\n",
    "         dec_padding_mask) = create_masks(inp, tar_inp)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions, _ = transformer(inp, tar_inp,\n",
    "                                         True,\n",
    "                                         enc_padding_mask,\n",
    "                                         combined_mask,\n",
    "                                         dec_padding_mask)\n",
    "            loss = loss_function(tar_real, predictions)\n",
    "\n",
    "        gradients = tape.gradient(loss, transformer.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients,\n",
    "                                      transformer.trainable_variables))\n",
    "\n",
    "        train_loss(loss)\n",
    "        train_accuracy(tar_real, predictions)\n",
    "\n",
    "\n",
    "    # def test_step(model, x_test, y_test):\n",
    "    #     predictions = model(x_test)\n",
    "    #     loss = loss_object(y_test, predictions)\n",
    "    #\n",
    "    #     test_loss(loss)\n",
    "    #     test_accuracy(y_test, predictions)\n",
    "\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        start = time.time()\n",
    "\n",
    "        train_loss.reset_states()\n",
    "        train_accuracy.reset_states()\n",
    "\n",
    "        train_dataset = data.batch_generator()\n",
    "        # inp -> portuguese, tar -> english\n",
    "        for (batch, (inp, tar)) in enumerate(train_dataset):\n",
    "            train_step(inp, tar)\n",
    "\n",
    "            ## Tensorboard event logging\n",
    "            ### QUESTION: should this be less often?\n",
    "            with train_summary_writer.as_default():\n",
    "                tf.summary.scalar('loss',\n",
    "                                  train_loss.result(),\n",
    "                                  step=epoch)\n",
    "                tf.summary.scalar('accuracy',\n",
    "                                  train_accuracy.result(),\n",
    "                                  step=epoch)\n",
    "\n",
    "            if batch % 50 == 0:\n",
    "                print ('Epoch {} Batch {} Loss {:.4f} Accuracy {:.4f}'.format(\n",
    "                       epoch + 1, batch, train_loss.result(), train_accuracy.result()))\n",
    "\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            ckpt_save_path = ckpt_manager.save()\n",
    "            print ('Saving checkpoint for epoch {} at {}'.format(epoch+1,\n",
    "                                                             ckpt_save_path))\n",
    "\n",
    "        print ('Epoch {} Loss {:.4f} Accuracy {:.4f}'.format(epoch + 1,\n",
    "                                                    train_loss.result(),\n",
    "                                                    train_accuracy.result()))\n",
    "\n",
    "        print ('Time taken for 1 epoch: {} secs\\n'.format(time.time() - start))\n",
    "\n",
    "    print(\"We got to here!!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _evaluate(inp_sentence, model):\n",
    "    transformer = model\n",
    "    \n",
    "#     start_token = [mnist_input_vocab_size]\n",
    "#     end_token = [mnist_input_vocab_size + 1]\n",
    "\n",
    "#     # inp sentence is portuguese, hence adding the start and end token\n",
    "#     inp_sentence = start_token + inp_sentence + end_token\n",
    "\n",
    "    encoder_input = tf.expand_dims(inp_sentence, 0)\n",
    "    print(encoder_input.shape)\n",
    "#     encoder_input = inp_sentence\n",
    "\n",
    "    # as the target is english, the first word to the transformer should be the\n",
    "    # english start token.\n",
    "    decoder_input = [mnist_target_vocab_size]\n",
    "    output = tf.expand_dims(decoder_input, 0)\n",
    "\n",
    "#     enc_padding_mask, combined_mask, dec_padding_mask = create_masks(\n",
    "#                                                             encoder_input, output)\n",
    "    \n",
    "    for i in range(MAX_LENGTH):\n",
    "#     for i in range(3):\n",
    "\n",
    "        if i % 20 == 0:\n",
    "            print(\"at step {} of {}\".format(i, MAX_LENGTH))\n",
    "        enc_padding_mask, combined_mask, dec_padding_mask = create_masks(encoder_input, output)\n",
    "\n",
    "        # predictions.shape == (batch_size, seq_len, vocab_size)\n",
    "        predictions, attention_weights = transformer(encoder_input, \n",
    "                                                     output,\n",
    "                                                     False,\n",
    "                                                     enc_padding_mask,\n",
    "                                                     combined_mask,\n",
    "                                                     dec_padding_mask)\n",
    "\n",
    "        # select the last word from the seq_len dimension\n",
    "        predictions = predictions[: ,-1:, :]  # (batch_size, 1, vocab_size)\n",
    "\n",
    "        predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
    "\n",
    "        # return the result if the predicted_id is equal to the end token\n",
    "        if predicted_id == mnist_target_vocab_size: # tokenizer_en.vocab_size+1:\n",
    "            return tf.squeeze(output, axis=0), attention_weights\n",
    "    \n",
    "        # concatentate the predicted_id to the output which is given to the decoder\n",
    "        # as its input.\n",
    "        output = tf.concat([output, predicted_id], axis=-1)\n",
    "\n",
    "    return tf.squeeze(output, axis=0), attention_weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Break down training function\n",
    "## Call transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Transformer\n"
     ]
    }
   ],
   "source": [
    "from src.hacked_transformer import Transformer\n",
    "from src.hacked_transformer import Encoder\n",
    "from src.hacked_transformer import Decoder\n",
    "\n",
    "\n",
    "MAX_LENGTH = config.MAX_LENGTH\n",
    "\n",
    "num_layers = config.num_layers\n",
    "d_model = 9 # config.d_model\n",
    "num_heads = 3 #config.num_heads\n",
    "dff = config.dff\n",
    "dropout_rate = config.dropout_rate\n",
    "\n",
    "mnist_input_vocab_size = 302\n",
    "mnist_target_vocab_size = 5\n",
    "\n",
    "# data = mnist_numpy_generator()\n",
    "# data = fake_data_gen()\n",
    "data = FakeData()\n",
    "batch_generator = data.batch_generator()\n",
    "\n",
    "print(\"Creating Transformer\")\n",
    "transformer = Transformer(num_layers, \n",
    "                          d_model, \n",
    "                          num_heads, \n",
    "                          dff,\n",
    "                          mnist_input_vocab_size, \n",
    "                          mnist_target_vocab_size,\n",
    "                          pe_input=800, # mnist_input_vocab_size,\n",
    "                          pe_target=5, # mnist_target_vocab_size,\n",
    "                          rate=dropout_rate)\n",
    "\n",
    "sample_item = next(batch_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## I can run a single sample through the Encoder!!! :D\n",
    "\n",
    "# temp_inp = sample_item[0][0]\n",
    "# temp_out = sample_item[0][1]\n",
    "\n",
    "# temp_inp = tf.expand_dims(temp_inp, 0)\n",
    "# temp_out = tf.expand_dims(temp_out, 0)\n",
    "\n",
    "# aa, bb, cc = create_masks(temp_inp, temp_out)\n",
    "\n",
    "# input_vocab_size = 302\n",
    "\n",
    "# temp_enc = Encoder(num_layers=3, \n",
    "#                    d_model=9, \n",
    "#                    num_heads=3, \n",
    "#                    dff=16, \n",
    "#                    input_vocab_size=input_vocab_size,\n",
    "# #                    target_vocab_size=11, \n",
    "#                    maximum_position_encoding=800, \n",
    "# #                    pe_target=3, \n",
    "#                    rate=0.1)\n",
    "# temp_enc(temp_inp, False, bb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1, 8)\n",
      "[[[9 9 4 4 2 7 9 6]]]\n",
      "(1, 1, 8, 4)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[[-0.01878595,  0.04641351,  0.03351081,  0.00574498],\n",
       "         [-0.01878595,  0.04641351,  0.03351081,  0.00574498],\n",
       "         [ 0.02236458, -0.03413758, -0.01948105, -0.04260851],\n",
       "         [ 0.02236458, -0.03413758, -0.01948105, -0.04260851],\n",
       "         [ 0.04038406, -0.02031455,  0.00128482, -0.03006921],\n",
       "         [-0.00227303, -0.00391444, -0.03188553, -0.03826999],\n",
       "         [-0.01878595,  0.04641351,  0.03351081,  0.00574498],\n",
       "         [-0.03483127,  0.00991713, -0.0410812 ,  0.01923412]]]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## What does the Embedding layer do? \n",
    "## can I replace it/get rid of it when using image data and/or a really small vocab?\n",
    "\n",
    "# temp_targ_voc_size = 10\n",
    "# temp_d_model = 4\n",
    "\n",
    "# temp_input = np.random.randint(0, 10, (1, 1, 8))\n",
    "# print(temp_input.shape)\n",
    "# print(temp_input)\n",
    "# # print(temp_input[0, 0, 4])\n",
    "\n",
    "# temp_out = tf.keras.layers.Embedding(temp_targ_voc_size, temp_d_model)(temp_input).numpy()\n",
    "# print(temp_out.shape)\n",
    "# temp_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preparing data ...\n",
      "Epoch 1 Batch 0 Loss 1.7454 Accuracy 0.2500\n",
      "Epoch 1 Loss 1.8157 Accuracy 0.1900\n",
      "Time taken for 1 epoch: 20.71367383003235 secs\n",
      "\n",
      "Epoch 2 Batch 0 Loss 1.7428 Accuracy 0.1500\n",
      "Epoch 2 Loss 1.7875 Accuracy 0.2100\n",
      "Time taken for 1 epoch: 0.29564595222473145 secs\n",
      "\n",
      "Epoch 3 Batch 0 Loss 1.7320 Accuracy 0.2000\n",
      "Epoch 3 Loss 1.7091 Accuracy 0.2200\n",
      "Time taken for 1 epoch: 0.3128628730773926 secs\n",
      "\n",
      "We got to here!!!\n"
     ]
    }
   ],
   "source": [
    "mnist_train_transformer(transformer, config, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30, 784) (30, 1)\n",
      "(784,) (1,)\n",
      "(1, 784)\n",
      "at step 0 of 400\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Incompatible shapes: [1,6,9] vs. [1,5,9] [Op:AddV2] name: transformer_19/decoder_18/add/",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-110-777ec6a14af9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0mtemp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Done\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-98-98a03fce04b3>\u001b[0m in \u001b[0;36m_evaluate\u001b[0;34m(inp_sentence, model)\u001b[0m\n\u001b[1;32m     33\u001b[0m                                                      \u001b[0menc_padding_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m                                                      \u001b[0mcombined_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m                                                      dec_padding_mask)\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;31m# select the last word from the seq_len dimension\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    889\u001b[0m           with base_layer_utils.autocast_context_manager(\n\u001b[1;32m    890\u001b[0m               self._compute_dtype):\n\u001b[0;32m--> 891\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    892\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tf2_test/notebooks/src/hacked_transformer.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inp, tar, training, enc_padding_mask, look_ahead_mask, dec_padding_mask)\u001b[0m\n\u001b[1;32m    364\u001b[0m         \u001b[0;31m# dec_output.shape == (batch_size, tar_seq_len, d_model)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m         dec_output, attention_weights = self.decoder(\n\u001b[0;32m--> 366\u001b[0;31m             tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n\u001b[0m\u001b[1;32m    367\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m         \u001b[0;31m# (batch_size, tar_seq_len, target_vocab_size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    889\u001b[0m           with base_layer_utils.autocast_context_manager(\n\u001b[1;32m    890\u001b[0m               self._compute_dtype):\n\u001b[0;32m--> 891\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    892\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tf2_test/notebooks/src/hacked_transformer.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, x, enc_output, training, look_ahead_mask, padding_mask)\u001b[0m\n\u001b[1;32m    325\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (batch_size, target_seq_len, d_model)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 327\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_encoding\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mseq_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/ops/math_ops.py\u001b[0m in \u001b[0;36mbinary_op_wrapper\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m    897\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    898\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 899\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    900\u001b[0m       \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSparseTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    901\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/ops/math_ops.py\u001b[0m in \u001b[0;36m_add_dispatch\u001b[0;34m(x, y, name)\u001b[0m\n\u001b[1;32m   1195\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1197\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1198\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1199\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36madd_v2\u001b[0;34m(x, y, name)\u001b[0m\n\u001b[1;32m    544\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    545\u001b[0m         \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 546\u001b[0;31m       \u001b[0m_six\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    547\u001b[0m   \u001b[0;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m   _, _, _op = _op_def_lib._apply_op_helper(\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Incompatible shapes: [1,6,9] vs. [1,5,9] [Op:AddV2] name: transformer_19/decoder_18/add/"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# # inp, target = next(data)\n",
    "inp, tar_inp = sample_item\n",
    "\n",
    "\n",
    "# (enc_padding_mask,\n",
    "#  combined_mask,\n",
    "#  dec_padding_mask) = create_masks(inp, tar_inp)\n",
    "\n",
    "print(inp.shape, tar_inp.shape)\n",
    "\n",
    "\n",
    "inp, tar_inp = inp[0], tar_inp[0]\n",
    "print(inp.shape, tar_inp.shape)\n",
    "\n",
    "# (enc_padding_mask,\n",
    "#  combined_mask,\n",
    "#  dec_padding_mask) = create_masks(inp, tar_inp)\n",
    "\n",
    "# print(enc_padding_mask.shape,\n",
    "#  combined_mask.shape,\n",
    "#  dec_padding_mask.shape)\n",
    "\n",
    "\n",
    "temp = _evaluate(inp, transformer)\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inp, target = next(data)\n",
    "inp, tar_inp = next(data)\n",
    "\n",
    "(enc_padding_mask,\n",
    " combined_mask,\n",
    " dec_padding_mask) = create_masks(inp, tar_inp)\n",
    "\n",
    "print(inp.shape, tar_inp.shape)\n",
    "\n",
    "print(enc_padding_mask.shape,\n",
    " combined_mask.shape,\n",
    " dec_padding_mask.shape)\n",
    "\n",
    "\n",
    "predictions, _ = transformer(inp, tar_inp,\n",
    "                             True,\n",
    "                             enc_padding_mask,\n",
    "                             combined_mask,\n",
    "                             dec_padding_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
